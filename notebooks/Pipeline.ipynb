{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5870d074",
   "metadata": {},
   "source": [
    "point your kernel at the project’s src/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c17666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Assume this notebook is in DTW-project/notebooks/\n",
    "project_root = Path().cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c410a9e",
   "metadata": {},
   "source": [
    "Core Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cb979e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy: 1.26.4\n",
      "Pandas: 2.3.1\n",
      "TensorFlow: 2.18.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "print(\"TensorFlow:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93857afb",
   "metadata": {},
   "source": [
    "define your data tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d848f2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_ROOT   = project_root / \"data\" / \"raw\"\n",
    "PROC_ROOT  = project_root / \"data\" / \"processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba13970c",
   "metadata": {},
   "source": [
    "----------------Data Ingestion & Catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec6f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1) skip if already done\n",
    "if (PROC_ROOT / \"biosecurid_catalog.parquet\").exists():\n",
    "    print(\"Catalog found—skipping ingest.\")\n",
    "else:\n",
    "    from src.io.load_biosecurid import build_catalog\n",
    "    df_catalog = build_catalog(\n",
    "        global_dir = PROC_ROOT / \"GlobalFeatures\",\n",
    "        local_dir  = RAW_ROOT  / \"LocalFunctions\",\n",
    "        output_catalog = project_root / \"data\" / \"biosecurid_catalog.parquet\"\n",
    "    )\n",
    "    print(\"Wrote\", len(df_catalog), \"entries\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fd6cf7",
   "metadata": {},
   "source": [
    "------------------Organise the dataset\n",
    "In a first notebook cell, use src/importData.py’s copy_subset or main() function to copy the raw BiosecurID files into a structured data/processed/uXXXX/… hierarchy\n",
    "GitHub\n",
    ". This only needs to be run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4109d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 2) Check for existing processed data ───\n",
    "processed = project_root / \"data\" / \"processed\"\n",
    "if any((processed / \"GlobalFeatures\").glob(\"u*.mat\")) and any((processed / \"LocalFunctions\").glob(\"u*.mat\")):\n",
    "    print(\"✅ Processed data already present; skipping import.\")\n",
    "else:\n",
    "    print(\"⏳ No processed files found; running importData.main() …\")\n",
    "    from src.data.importData import main as import_main\n",
    "    import_main()\n",
    "    print(\"✅ Data import complete.\")\n",
    "\n",
    "# ─── 3) Quick sanity‐check of the directory tree ───\n",
    "for user_dir in sorted((processed).glob(\"u*\")):\n",
    "    print(f\"\\n{user_dir.relative_to(project_root)}\")\n",
    "    print(\"  GlobalFeatures:\", len(list((user_dir/\"GlobalFeatures\").glob(\"*.mat\"))), \"files\")\n",
    "    print(\"  LocalFunctions:\",  len(list((user_dir/\"LocalFunctions\").glob(\"*.mat\"))),  \"files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e4186f",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde19700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# 2.1) load catalog\n",
    "cat = pd.read_parquet(project_root/\"data\"/\"biosecurid_catalog.parquet\")\n",
    "\n",
    "# 2.2) global‐feature EDA\n",
    "gf = pd.DataFrame(\n",
    "    [np.loadtxt(p) for p in cat.global_path],  # or use load_global()\n",
    "    columns=[f\"f{i+1}\" for i in range(40)]\n",
    ")\n",
    "sns.histplot(data=gf, kde=True)\n",
    "plt.title(\"All 40 Global Features\")\n",
    "\n",
    "# 2.3) correlation heatmap\n",
    "corr = gf.corr()\n",
    "sns.heatmap(corr, vmax=1, vmin=-1, center=0)\n",
    "plt.title(\"Pearson Correlation of Global Features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf1a21",
   "metadata": {},
   "source": [
    "Pair-Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0311082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pairing.make_pairs import generate_pairs as make_pairs\n",
    "# make_pairs reads your catalog, builds genuine vs forgery pairs, subsamples,\n",
    "# and writes out pairs_meta.parquet\n",
    "cat = pd.read_parquet(project_root/\"data\"/\"biosecurid_catalog.parquet\")\n",
    "pairs_meta = make_pairs(\n",
    "    df = cat\n",
    ")\n",
    "print(\"Generated\", len(pairs_meta), \"pairs\")\n",
    "pairs_meta.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af5ef2b",
   "metadata": {},
   "source": [
    "DTW Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daaf0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dtw.compute_dtw import build_cache\n",
    "\n",
    "# build_cache will read pairs_meta, compute dp distances (via dtaidistance or your dp),\n",
    "# normalise them, and write dtw_cache.parquet\n",
    "dtw_cache = build_cache(\n",
    "    pairs_path = project_root/\"data\"/\"pairs_meta.parquet\",\n",
    "    catalog_path = project_root/\"data\"/\"biosecurid_catalog.parquet\",\n",
    "    cache_path = project_root/\"data\"/\"dtw_cache.parquet\",\n",
    ")\n",
    "\n",
    "if dtw_cache is not None:\n",
    "    print(\"Cached DTW for\", len(dtw_cache), \"pairs\")\n",
    "else:\n",
    "    print(\"DTW cache built and saved, but no object returned.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d57781",
   "metadata": {},
   "source": [
    "Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636b3c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# load\n",
    "pairs = pd.read_parquet(project_root/\"data\"/\"pairs_meta.parquet\")\n",
    "cache = pd.read_parquet(project_root/\"data\"/\"dtw_cache.parquet\")\n",
    "df = pairs.merge(cache, on=\"pair_id\")\n",
    "\n",
    "# ROC & AUC\n",
    "fpr, tpr, thr = roc_curve(df.label, df.d_norm1)\n",
    "auc = roc_auc_score(df.label, df.d_norm1)\n",
    "\n",
    "plt.plot(fpr, tpr, label=f\"AUC={auc:.3f}\")\n",
    "plt.plot([0,1],[0,1],\"k--\")\n",
    "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\")\n",
    "plt.title(\"Baseline ROC Curve\"); plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DTW-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
