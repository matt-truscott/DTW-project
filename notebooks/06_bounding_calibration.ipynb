{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecd5d238",
   "metadata": {},
   "source": [
    "## 6) Bounded DTW & Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d4bce2",
   "metadata": {},
   "source": [
    "- **Raw** DTW distances normalized by various methods → compute AUC (Area Under the Curve) & EER (Equal Error Rate) \n",
    "- **Calibrate** the best normalization (`d_by_avg_len`) via logistic regression  \n",
    "- Report both raw and calibrated AUC/EER, save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c01153f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc</th>\n",
       "      <th>eer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d_by_path</th>\n",
       "      <td>0.483249</td>\n",
       "      <td>0.518568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d_by_ref_len</th>\n",
       "      <td>0.117139</td>\n",
       "      <td>0.798177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d_by_qry_len</th>\n",
       "      <td>0.537408</td>\n",
       "      <td>0.476328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d_by_avg_len</th>\n",
       "      <td>0.310429</td>\n",
       "      <td>0.647604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   auc       eer\n",
       "d_by_path     0.483249  0.518568\n",
       "d_by_ref_len  0.117139  0.798177\n",
       "d_by_qry_len  0.537408  0.476328\n",
       "d_by_avg_len  0.310429  0.647604"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw     d_by_avg_len → AUC 0.3104, EER 0.6476\n",
      "Calibrated      → AUC 0.6922, EER 0.3470 @ threshold 0.5247\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_auc</th>\n",
       "      <th>raw_eer</th>\n",
       "      <th>calibrated_auc</th>\n",
       "      <th>calibrated_eer</th>\n",
       "      <th>calibrated_thresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.310429</td>\n",
       "      <td>0.647604</td>\n",
       "      <td>0.692245</td>\n",
       "      <td>0.347005</td>\n",
       "      <td>0.524671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    raw_auc   raw_eer  calibrated_auc  calibrated_eer  calibrated_thresh\n",
       "0  0.310429  0.647604        0.692245        0.347005           0.524671"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# make sure `src/` is importable\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# helpers\n",
    "from src.evaluation.evaluation import load_results, compute_metrics, plot_roc, plot_det\n",
    "from src.calibration.calibration import add_normalizations, train_calibrator\n",
    "\n",
    "\n",
    "# paths\n",
    "PAIRS_PATH  = project_root/\"data\"/\"pairs_meta.parquet\"\n",
    "CACHE_PATH  = project_root/\"data\"/\"dtw_cache.parquet\"\n",
    "FIG_DIR     = project_root/\"figures\"\n",
    "RESULTS_DIR = project_root/\"results\"\n",
    "FIG_DIR.mkdir(exist_ok=True, parents=True)\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# 6.1) Load & merge\n",
    "df = load_results(PAIRS_PATH, CACHE_PATH)\n",
    "\n",
    "# 6.2) Add four simple normalizations\n",
    "df_norm = add_normalizations(df)\n",
    "\n",
    "# 6.3) Evaluate each normalization’s AUC & EER\n",
    "methods = ['d_by_path','d_by_ref_len','d_by_qry_len','d_by_avg_len']\n",
    "norm_results = {}\n",
    "for m in methods:\n",
    "    res = compute_metrics(df_norm, score_col=m)\n",
    "    norm_results[m] = {'auc': res['auc'], 'eer': res['eer']}\n",
    "# display table of raw performance\n",
    "display(pd.DataFrame(norm_results).T)\n",
    " \n",
    "# 6.4) Plot ROC for the best normalization (here 'd_by_avg_len')\n",
    "best = 'd_by_avg_len'\n",
    "res_best = compute_metrics(df_norm, score_col=best)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "plot_roc(res_best['fpr'], res_best['tpr'], res_best['auc'], ax=ax)\n",
    "ax.set_title(f\"Raw ROC ({best})\")\n",
    "fig.savefig(FIG_DIR/f\"roc_{best}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "# 6.5) Train & evaluate a logistic calibrator on that best feature\n",
    "model, X_test, y_test, y_score_test = train_calibrator(\n",
    "    df_norm, feature_col=best, method='logistic'\n",
    ")\n",
    "\n",
    "# calibrated ROC & AUC\n",
    "fpr_c, tpr_c, thr_c = roc_curve(y_test, y_score_test)\n",
    "auc_c = roc_auc_score(y_test, y_score_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(fpr_c, tpr_c, label=f\"AUC={auc_c:.3f}\", lw=2)\n",
    "ax.plot([0,1],[0,1],'--',color='gray')\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "ax.set_title(f\"Calibrated ROC ({best})\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "fig.savefig(FIG_DIR/f\"roc_calibrated_{best}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "# 6.6) Compute calibrated EER & report summary\n",
    "fnr_c = 1 - tpr_c\n",
    "idx_c = np.argmin(np.abs(fpr_c - fnr_c))\n",
    "eer_c = float((fpr_c[idx_c] + fnr_c[idx_c]) / 2)\n",
    "thr_eer_c = float(thr_c[idx_c])\n",
    "\n",
    "raw_auc = norm_results[best]['auc']\n",
    "raw_eer = norm_results[best]['eer']\n",
    "\n",
    "print(f\"Raw     {best} → AUC {raw_auc:.4f}, EER {raw_eer:.4f}\")\n",
    "print(f\"Calibrated      → AUC {auc_c:.4f}, EER {eer_c:.4f} @ threshold {thr_eer_c:.4f}\")\n",
    "\n",
    "# 6.7) Save all scalar metrics to JSON\n",
    "calibrated_metrics = {\n",
    "    \"raw_auc\":          raw_auc,\n",
    "    \"raw_eer\":          raw_eer,\n",
    "    \"calibrated_auc\":   auc_c,\n",
    "    \"calibrated_eer\":   eer_c,\n",
    "    \"calibrated_thresh\": thr_eer_c\n",
    "}\n",
    "with open(RESULTS_DIR/\"calibrated_metrics.json\", \"w\") as f:\n",
    "    json.dump(calibrated_metrics, f, indent=2)\n",
    "\n",
    "# Optional: show saved metrics\n",
    "display(pd.DataFrame([calibrated_metrics]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DTW-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
