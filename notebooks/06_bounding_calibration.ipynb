{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecd5d238",
   "metadata": {},
   "source": [
    "## 6) Bounded DTW & Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d4bce2",
   "metadata": {},
   "source": [
    "- **Raw** DTW distances normalized by various methods → compute AUC & EER  \n",
    "- **Calibrate** the best normalization (`d_by_avg_len`) via logistic regression  \n",
    "- Report both raw and calibrated AUC/EER, save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c01153f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw     d_by_avg_len AUC = 0.3104, EER = 0.6476\n",
      "Calibrated      AUC = 0.6922\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# make sure `src/` is importable\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# helpers\n",
    "from src.evaluation.evaluation import load_results, compute_metrics, plot_roc, plot_det\n",
    "from src.calibration.calibration import add_normalizations, train_calibrator\n",
    "\n",
    "\n",
    "# paths\n",
    "PAIRS_PATH  = project_root/\"data\"/\"pairs_meta.parquet\"\n",
    "CACHE_PATH  = project_root/\"data\"/\"dtw_cache.parquet\"\n",
    "FIG_DIR     = project_root/\"figures\"\n",
    "RESULTS_DIR = project_root/\"results\"\n",
    "FIG_DIR.mkdir(exist_ok=True, parents=True)\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# 6.1) Load & merge\n",
    "df = load_results(PAIRS_PATH, CACHE_PATH)\n",
    "\n",
    "# 6.2) Add four simple normalizations\n",
    "df_norm = add_normalizations(df)\n",
    "\n",
    "# 6.3) Evaluate each normalization’s AUC & EER\n",
    "methods = ['d_by_path','d_by_ref_len','d_by_qry_len','d_by_avg_len']\n",
    "norm_results = {}\n",
    "for m in methods:\n",
    "    res = compute_metrics(df_norm, score_col=m)\n",
    "    norm_results[m] = {'auc': res['auc'], 'eer': res['eer']}\n",
    "# display table of raw performance\n",
    "display(pd.DataFrame(norm_results).T)\n",
    " \n",
    "# 6.4) Plot ROC for the best normalization (here 'd_by_avg_len')\n",
    "best = 'd_by_avg_len'\n",
    "res_best = compute_metrics(df_norm, score_col=best)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "plot_roc(res_best['fpr'], res_best['tpr'], res_best['auc'], ax=ax)\n",
    "ax.set_title(f\"Raw ROC ({best})\")\n",
    "fig.savefig(FIG_DIR/f\"roc_{best}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "# 6.5) Train & evaluate a logistic calibrator on that best feature\n",
    "model, X_test, y_test, y_score_test = train_calibrator(\n",
    "    df_norm, feature_col=best, method='logistic'\n",
    ")\n",
    "\n",
    "# calibrated ROC & AUC\n",
    "fpr_c, tpr_c, thr_c = roc_curve(y_test, y_score_test)\n",
    "auc_c = roc_auc_score(y_test, y_score_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(fpr_c, tpr_c, label=f\"AUC={auc_c:.3f}\", lw=2)\n",
    "ax.plot([0,1],[0,1],'--',color='gray')\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "ax.set_title(f\"Calibrated ROC ({best})\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "fig.savefig(FIG_DIR/f\"roc_calibrated_{best}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "# 6.6) Compute calibrated EER & report summary\n",
    "fnr_c = 1 - tpr_c\n",
    "idx_c = np.argmin(np.abs(fpr_c - fnr_c))\n",
    "eer_c = float((fpr_c[idx_c] + fnr_c[idx_c]) / 2)\n",
    "thr_eer_c = float(thr_c[idx_c])\n",
    "\n",
    "raw_auc = norm_results[best]['auc']\n",
    "raw_eer = norm_results[best]['eer']\n",
    "\n",
    "print(f\"Raw     {best} → AUC {raw_auc:.4f}, EER {raw_eer:.4f}\")\n",
    "print(f\"Calibrated      → AUC {auc_c:.4f}, EER {eer_c:.4f} @ threshold {thr_eer_c:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DTW-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
