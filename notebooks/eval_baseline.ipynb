{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "cells": [
  {
   "id": "452db3f5",
   "cell_type": "markdown",
   "source": "# Baseline Evaluation\nThis notebook computes baseline DTW verification performance using a simple global threshold rule.",
   "metadata": {}
  },
  {
   "id": "50b48fbe",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, roc_auc_score\nimport pathlib",
   "outputs": []
  },
  {
   "id": "278f7179",
   "cell_type": "markdown",
   "source": "## Load and merge data",
   "metadata": {}
  },
  {
   "id": "7dd0c4f5",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "pairs = pd.read_parquet('data/pairs_meta.parquet')\ndtw = pd.read_parquet('data/dtw_cache.parquet')\ndf = pairs.merge(dtw, on='pair_id')\nlabel_map = {'genuine': 1, 'forgery': 0}\ny = df['label'].map(label_map).values\ndistance_cols = ['d_raw', 'd_norm1', 'd_norm2']",
   "outputs": []
  },
  {
   "id": "f1a6b1dd",
   "cell_type": "markdown",
   "source": "## Helper functions",
   "metadata": {}
  },
  {
   "id": "a28b088a",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "def compute_metrics(scores, labels):\n    fpr, tpr, thr = roc_curve(labels, -scores)\n    auc = roc_auc_score(labels, -scores)\n    fnr = 1 - tpr\n    idx = np.nanargmin(np.abs(fnr - fpr))\n    eer = (fpr[idx] + fnr[idx]) / 2\n    eer_thr = thr[idx]\n    return fpr, tpr, auc, eer, eer_thr",
   "outputs": []
  },
  {
   "id": "4b077572",
   "cell_type": "markdown",
   "source": "## Global evaluation",
   "metadata": {}
  },
  {
   "id": "f43b3dc2",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "results = []\nroc_data = {}\nfor col in distance_cols:\n    fpr, tpr, auc, eer, thr = compute_metrics(df[col].values, y)\n    results.append({'distance': col, 'eer': eer, 'auc': auc})\n    roc_data[col] = (fpr, tpr)\nmetrics_df = pd.DataFrame(results)\nmetrics_df.to_csv('results/baseline_metrics.csv', index=False)\nmetrics_df",
   "outputs": []
  },
  {
   "id": "7c0093bb",
   "cell_type": "markdown",
   "source": "## Plot ROC and DET curves",
   "metadata": {}
  },
  {
   "id": "60a87601",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "fig, ax = plt.subplots(1, 2, figsize=(12,5))\nfor col, (fpr, tpr) in roc_data.items():\n    ax[0].plot(fpr, tpr, label=col)\n    fnr = 1 - tpr\n    ax[1].plot(fpr, fnr, label=col)\nax[0].set_title('ROC')\nax[0].set_xlabel('FPR')\nax[0].set_ylabel('TPR')\nax[1].set_title('DET')\nax[1].set_xlabel('FPR')\nax[1].set_ylabel('FNR')\nfor a in ax:\n    a.legend()\nfig.tight_layout()\npathlib.Path('figures').mkdir(exist_ok=True)\nfig.savefig('figures/baseline_curves.png')\nfig",
   "outputs": []
  },
  {
   "id": "77991224",
   "cell_type": "markdown",
   "source": "## Per-user threshold analysis",
   "metadata": {}
  },
  {
   "id": "a32140f8",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "user_metrics = []\nfor user, grp in df.groupby('userA'):\n    entry = {'user': user}\n    for col in distance_cols:\n        _, _, _, eer, _ = compute_metrics(grp[col].values, grp['label'].map(label_map).values)\n        entry[col] = eer\n    user_metrics.append(entry)\nuser_df = pd.DataFrame(user_metrics)\nuser_df.describe()[distance_cols]",
   "outputs": []
  },
  {
   "id": "d13a6833",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "user_df[distance_cols].hist(bins=20, figsize=(10,4), layout=(1,3))\nplt.tight_layout()\nplt.savefig('figures/user_eer_hist.png')\nplt.show()",
   "outputs": []
  }
 ]
}